# ğŸ¦™ OpenHoof

**Agentic AI that kicks into action.**

<p align="center">
  <img src="docs/openhoof-logo.png" alt="OpenHoof - A cool llama with sunglasses and a bedazzled hoof" width="300">
</p>

> *"Why have claws when you can have hooves?"*  
> â€” Ancient LlamaFarm Proverb

OpenHoof is a standalone, extensible platform for running AI agents that persist across sessions, respond to events, and coordinate with each other. Built to work with [LlamaFarm](https://github.com/llama-farm/llamafarm) for local inference, but adaptable to any LLM backend.

Some say it was inspired by a certain [claw-based project](https://github.com/anthropics/claude-code)... but we believe hooves are simply more elegant. Plus, llamas don't scratch â€” they *kick*.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     YOUR APPLICATION                        â”‚
â”‚              (HORIZON, Medical Wing, etc.)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                  â”‚
â”‚                    Trigger API                              â”‚
â”‚                     (webhook)                               â”‚
â”‚                          â–¼                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      O P E N H O O F                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Fuel   â”‚  â”‚  Intel  â”‚  â”‚   MX    â”‚  â”‚ Orchestrâ”‚        â”‚
â”‚  â”‚ Analyst â”‚  â”‚ Analyst â”‚  â”‚Specialistâ”‚ â”‚  -ator  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚            â”‚            â”‚            â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                  â”‚
â”‚                    LlamaFarm                                â”‚
â”‚                  (local inference)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

- **ğŸ¯ Event-Driven Agents** â€” External systems fire webhooks, agents wake up with full context
- **ğŸ§  Persistent Memory** â€” Agents remember across sessions via workspace files (SOUL.md, MEMORY.md)
- **ğŸ‘¥ Multi-Agent Coordination** â€” Orchestrator agents spawn specialists as needed
- **ğŸ”§ Extensible Tools** â€” Plugin architecture for custom capabilities
- **ğŸ–¥ï¸ Web Dashboard** â€” Monitor agents, review activity, approve actions
- **ğŸ”Œ LlamaFarm Integration** â€” Works with any LlamaFarm project for inference
- **ğŸ“¡ Real-time Events** â€” WebSocket streaming for live updates
- **ğŸ§  FunctionGemma Training** â€” LoRA fine-tune a 270M router for your tools in 3 minutes, cross-platform
- **ğŸ¦™ 100% More Llama** â€” No claws required

## ğŸ§  FunctionGemma Tool Router â€” Train Your Own

OpenHoof includes a **two-stage function calling pipeline** powered by [FunctionGemma-270M](https://huggingface.co/google/functiongemma-270m-it). A tiny 270M model handles ALL tool routing in <300ms â€” then your bigger model only reasons about results.

```
User: "Save a note about the meeting"
         â†“
  FunctionGemma-270M  (550MB RAM, ~270ms)
         â†“
  [{"name": "memory_write", ...}]
         â†“
  Tool executes â†’ Result fed to bigger model
```

### Why This Matters

| Model | Accuracy | Latency | Notes |
|-------|----------|---------|-------|
| FunctionGemma-270M (base) | 15.4% | 203ms | Needs fine-tuning |
| Qwen3-1.7B (as router) | 38.5% | 515ms | Too slow for edge |
| **FunctionGemma-270M (LoRA)** | **100%** | **271ms** | ğŸ”¥ 3 min training |

### Train in One Command

```bash
# Mac (Apple Silicon) â€” uses unsloth-mlx
python training/train_tool_router.py

# Linux (CUDA) â€” uses unsloth
python training/train_tool_router.py --backend cuda
```

Same script, cross-platform. LoRA fine-tuning (rank 16, alpha 32) across all 18 layers. Trains on ~226 examples in ~3 minutes on Apple Silicon. Peak memory: 2.1GB.

### Continuous Learning

Every tool routing decision is logged. The model improves as you use it:

```bash
# Generate synthetic training data (uses your bigger model as teacher)
python training/pipeline.py generate --count 50

# Retrain with new data
python training/pipeline.py run

# Auto-retrain only when enough new data exists
python training/train_tool_router.py --auto --min-examples 200
```

The pipeline collects live routing decisions, tracks outcome feedback, and retrains on demand. Exports to GGUF for deployment via LlamaFarm or llama.cpp.

> **Full details:** [`training/README.md`](training/README.md) Â· **Pipeline code:** [`openhoof/inference/function_pipeline.py`](openhoof/inference/function_pipeline.py)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+ (for web UI)
- [LlamaFarm](https://github.com/llama-farm/llamafarm) running on `localhost:14345`

### Installation

```bash
# Clone the repo
git clone https://github.com/llama-farm/openhoof.git
cd openhoof

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -e .

# Initialize workspace
openhoof init
```

### Start the Server

```bash
# Start OpenHoof API (default: port 18765)
openhoof start

# In another terminal, start the web UI (default: port 13456)
cd ui && npm install && npm run dev
```

### Verify It's Running

```bash
curl http://localhost:18765/api/health
# {"status":"healthy","components":{"api":true,"inference":true}}
```

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [Getting Started](docs/GETTING_STARTED.md) | First steps with OpenHoof |
| [Architecture](docs/ARCHITECTURE.md) | How the system works |
| [Triggers](docs/TRIGGERS.md) | Event-driven agent spawning |
| [Agents](docs/AGENTS.md) | Creating and managing agents |
| [Tools](docs/TOOLS.md) | Extending agent capabilities |
| [LlamaFarm Integration](docs/LLAMAFARM.md) | Connecting to local LLMs |
| [API Reference](docs/API.md) | REST API documentation |

## ğŸ¯ Example: Event-Driven Agent

When your application detects an anomaly, fire a webhook to OpenHoof:

```bash
curl -X POST http://localhost:18765/api/triggers \
  -H "Content-Type: application/json" \
  -d '{
    "source": "horizon",
    "event_type": "anomaly",
    "category": "fuel",
    "severity": "warning",
    "title": "Fuel Burn Rate Deviation",
    "description": "Current burn rate is 15% above planned",
    "data": {
      "burn_ratio": 1.15,
      "current_fuel_lbs": 145000
    }
  }'
```

Response:
```json
{
  "trigger_id": "TRG-20260206-0001",
  "status": "spawned",
  "agent_id": "fuel-analyst",
  "session_id": "abc123..."
}
```

The `fuel-analyst` agent wakes up, analyzes the situation, and can:
- Query its knowledge base
- Spawn sub-agents for specialized analysis
- Queue notifications for human approval
- Update its memory for future reference

## ğŸ—ï¸ Project Structure

```
openhoof/
â”œâ”€â”€ openhoof/                 # Python package
â”‚   â”œâ”€â”€ api/                  # FastAPI routes
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ agents.py     # Agent CRUD
â”‚   â”‚       â”œâ”€â”€ chat.py       # Chat interface
â”‚   â”‚       â””â”€â”€ triggers.py   # Event triggers
â”‚   â”œâ”€â”€ agents/               # Agent lifecycle
â”‚   â”œâ”€â”€ core/                 # Sessions, events, workspace
â”‚   â”œâ”€â”€ inference/            # LlamaFarm adapter
â”‚   â””â”€â”€ tools/                # Built-in tools
â”œâ”€â”€ ui/                       # Next.js web dashboard
â”œâ”€â”€ integrations/             # Drop-in clients for apps
â”‚   â”œâ”€â”€ atmosphere_client.py  # Python SDK
â”‚   â”œâ”€â”€ horizon_integration.py
â”‚   â””â”€â”€ medical_wing_integration.py
â”œâ”€â”€ docs/                     # Documentation
â””â”€â”€ examples/                 # Example agents and configs
```

## ğŸ”§ Configuration

OpenHoof uses `~/.openhoof/config.yaml`:

```yaml
# API settings
api:
  host: 0.0.0.0
  port: 18765
  cors_origins:
    - http://localhost:13456

# LlamaFarm connection
inference:
  base_url: http://localhost:14345
  namespace: default
  project: openhoof

# Auto-start these agents on boot
autostart_agents:
  - orchestrator
```

## ğŸ¤ Integration Examples

### Python (Async)
```python
from openhoof import OpenHoofClient

client = OpenHoofClient("http://localhost:18765")

# Fire a trigger
response = await client.trigger(
    source="my-app",
    event_type="alert",
    severity="warning",
    title="Something happened",
    data={"details": "..."}
)

print(f"Agent {response.agent_id} is handling it")
```

### Python (Callback for Anomaly Detectors)
```python
from openhoof import AnomalyTriggerCallback

# Register with your anomaly engine
callback = AnomalyTriggerCallback(source="my-app")
my_detector.register_callback(callback)

# Now anomalies automatically trigger agents!
```

### cURL / Any HTTP Client
```bash
curl -X POST http://localhost:18765/api/triggers \
  -H "Content-Type: application/json" \
  -d '{"source":"my-app","event_type":"alert","title":"Help!"}'
```

## ğŸ§© Extending OpenHoof

### Custom Tools

Create tools that agents can use:

```python
# tools/my_tool.py
from openhoof.tools import Tool, ToolResult

class WeatherTool(Tool):
    name = "get_weather"
    description = "Get current weather for a location"
    
    async def execute(self, location: str) -> ToolResult:
        # Your implementation
        return ToolResult(success=True, data={"temp": 72})
```

### Custom Trigger Rules

Add routing rules for your application:

```bash
curl -X POST http://localhost:18765/api/triggers/rules \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-critical-handler",
    "source": "my-app",
    "event_type": "*",
    "min_severity": "critical",
    "agent_id": "emergency-responder"
  }'
```

### Agent Templates

Create agent templates for quick deployment:

```yaml
# templates/analyst.yaml
name: "{{domain}}-analyst"
soul: |
  You are a {{domain}} analyst AI.
  Your job is to analyze {{domain}} data and provide insights.
tools:
  - search
  - calculate
  - notify
```

## ğŸŒŸ Why "OpenHoof"?

- **Open** â€” Open source, extensible, integrates with anything
- **Hoof** â€” Agents that "kick" into action (llamas kick, they don't claw ğŸ¦™)
- Part of the [LlamaFarm](https://github.com/llama-farm) ecosystem

### A Note on Claws vs Hooves

You may have heard of [Claude Code](https://github.com/anthropics/claude-code) (nÃ©e OpenClaw), Anthropic's excellent coding agent. Great project! Sharp claws! Very pointy!

But consider: **claws scratch**. They're for climbing trees and looking threatening.

**Hooves**, on the other hand, are for *getting things done*. Llamas carry cargo across mountains. They kick predators into next week. They look fabulous doing it.

Plus, our hooves are *bedazzled*. âœ¨

## ğŸ“œ License

Apache 2.0

## ğŸ™ Acknowledgments

- [Claude Code](https://github.com/anthropics/claude-code) â€” The workspace/agent patterns that inspired this (we come in peace ğŸ¦™ğŸ¤ğŸ»)
- [LlamaFarm](https://github.com/llama-farm/llamafarm) â€” Local LLM inference
- Built with â¤ï¸ for the Air Force and anyone who needs reliable local AI agents

---

**Ready to let your agents kick into action?** [Get Started â†’](docs/GETTING_STARTED.md)

*No llamas were harmed in the making of this framework. Several were bedazzled.*
